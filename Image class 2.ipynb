{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the modules\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "import os\n",
    "\n",
    "input_output_folder1 = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\train\\\\Covid\"\n",
    "input_output_folder2 = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\train\\\\Normal\"\n",
    "input_output_folder3 = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\train\\\\Viral Pneumonia\"\n",
    "\n",
    "\n",
    "p1 = Augmentor.Pipeline(source_directory=input_output_folder1, output_directory=input_output_folder1)\n",
    "p2 = Augmentor.Pipeline(source_directory=input_output_folder2, output_directory=input_output_folder2)\n",
    "p3 = Augmentor.Pipeline(source_directory=input_output_folder3, output_directory=input_output_folder3)\n",
    "\n",
    "#defining augmentation operations\n",
    "\n",
    "augmentation_operations = [\n",
    "    lambda p: p.rotate(probability=0.9, max_left_rotation=25, max_right_rotation=25),\n",
    "    lambda p: p.random_distortion(probability=0.8, grid_width=3, grid_height=3, magnitude=16),\n",
    "    lambda p: p.random_erasing(probability=0.7, rectangle_area=0.4),\n",
    "    lambda p: p.flip_left_right(probability=0.8),\n",
    "    lambda p: p.flip_top_bottom(probability=0.8),\n",
    "    lambda p: p.zoom_random(probability=0.8, percentage_area=0.6),\n",
    "    lambda p: p.random_contrast(probability=0.7, min_factor=0.5, max_factor=1.5),\n",
    "    lambda p: p.random_brightness(probability=0.7, min_factor=0.5, max_factor=1.5),\n",
    "    lambda p: p.skew(probability=0.6, magnitude=0.6),\n",
    "    lambda p: p.shear(probability=0.6, max_shear_left=20, max_shear_right=20),\n",
    "    lambda p: p.random_color(probability=0.5, min_factor=0.5, max_factor=2.0)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for operation in augmentation_operations:\n",
    "    operation(p1)\n",
    "    operation(p2)\n",
    "    operation(p3)\n",
    "\n",
    "p1.set_save_format(\"png\")\n",
    "p2.set_save_format(\"png\")\n",
    "p3.set_save_format(\"png\")\n",
    "\n",
    "num_samples = 300\n",
    "\n",
    "p1.sample(num_samples)\n",
    "p2.sample(num_samples)\n",
    "p3.sample(num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 1110\n",
      "Shape of x_train: (1110, 224, 224, 3)\n",
      "Shape of y_train: (1110,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#importing the training dataset\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "covid_dir = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\train\\\\Covid\"\n",
    "normal_dir = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\train\\\\Normal\"\n",
    "viral_dir = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\train\\\\Viral Pneumonia\"\n",
    "\n",
    "#initialising the lists for images and labels\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "#function to load the images and assign a label\n",
    "\n",
    "def load_images(folder, label):\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)  \n",
    "        if img is not None: \n",
    "            img = cv2.resize(img, (224, 224)) \n",
    "            x_train.append(img) \n",
    "            y_train.append(label)\n",
    "\n",
    "\n",
    "# Loading images from directory and assigning labels, 0 for covid, 1 for normal, 2 for viral pneumonia\n",
    "\n",
    "load_images(covid_dir, 0)\n",
    "load_images(normal_dir, 1)\n",
    "load_images(viral_dir, 2)\n",
    "\n",
    "# Converting lists to numpy arrays\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(f\"Total images loaded: {len(x_train)}\")\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images loaded: 66\n",
      "Shape of x_test: (66, 224, 224, 3)\n",
      "Shape of y_test: (66,)\n"
     ]
    }
   ],
   "source": [
    "#importing the testing dataset\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "covid_dir1 = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\test\\\\Covid\"\n",
    "normal_dir1 = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\test\\\\Normal\"\n",
    "viral_dir1 = \"D:\\\\Varun Robo 2\\\\Covid19-dataset\\\\test\\\\Viral Pneumonia\"\n",
    "\n",
    "\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# Function to load images and assign labels\n",
    "\n",
    "def load_images(folder, label):\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        img = cv2.imread(img_path)  \n",
    "        if img is not None: \n",
    "            img = cv2.resize(img, (224, 224)) \n",
    "            x_test.append(img)\n",
    "            y_test.append(label)\n",
    "\n",
    "# Assigning labels 0 for covid images,1 for normal images, 2 for viral pneumonia images\n",
    "\n",
    "load_images(covid_dir1, 0)  \n",
    "load_images(normal_dir1, 1)\n",
    "load_images(viral_dir1, 2)\n",
    "\n",
    "# Converting the lists to numpy arrays\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Total images loaded: {len(x_test)}\")\n",
    "print(f\"Shape of x_test: {x_test.shape}\")\n",
    "print(f\"Shape of y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train after normalization: (1110, 224, 224, 3)\n",
      "Shape of x_test after normalization: (66, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#normalising the data\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "print(f\"Shape of x_train after normalization: {x_train.shape}\")\n",
    "print(f\"Shape of x_test after normalization: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (888, 224, 224, 3)\n",
      "Shape of x_val: (222, 224, 224, 3)\n",
      "Shape of y_train: (888,)\n",
      "Shape of y_val: (222,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Perform the 80/20 split, val dataset is 20 percent of the train dataset\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of x_val: {x_val.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "print(f\"Shape of y_val: {y_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers \n",
    "\n",
    "def create_model(lr=0.001): \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(3, activation='softmax', kernel_regularizer=regularizers.l2(0.001))\n",
    "    ])\n",
    "\n",
    "   \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "   \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 3s/step - accuracy: 0.4549 - loss: 3.0357 - val_accuracy: 0.4009 - val_loss: 2.2858\n",
      "Epoch 2/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 2s/step - accuracy: 0.6261 - loss: 2.3617 - val_accuracy: 0.2973 - val_loss: 2.5284\n",
      "Epoch 3/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.6924 - loss: 1.8085 - val_accuracy: 0.3153 - val_loss: 2.8783\n",
      "Epoch 4/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.7701 - loss: 1.5146 - val_accuracy: 0.3649 - val_loss: 3.4264\n",
      "Epoch 5/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.8182 - loss: 1.4031 - val_accuracy: 0.4550 - val_loss: 3.3347\n",
      "Epoch 6/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.8992 - loss: 1.2170 - val_accuracy: 0.3604 - val_loss: 4.1358\n",
      "Epoch 7/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9232 - loss: 1.1160 - val_accuracy: 0.3604 - val_loss: 4.3090\n",
      "Epoch 8/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9273 - loss: 1.0608 - val_accuracy: 0.3604 - val_loss: 6.2109\n",
      "Epoch 9/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 2s/step - accuracy: 0.9592 - loss: 0.9723 - val_accuracy: 0.3604 - val_loss: 4.8902\n",
      "Epoch 10/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 2s/step - accuracy: 0.9582 - loss: 0.9777 - val_accuracy: 0.3649 - val_loss: 5.3666\n"
     ]
    }
   ],
   "source": [
    "model = create_model(learning_rate=0.0001)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs=10,\n",
    "                    batch_size=32, \n",
    "                    validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 952ms/step - accuracy: 0.3849 - loss: 2.2821 - val_accuracy: 0.7252 - val_loss: 0.6603\n",
      "Epoch 2/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 654ms/step - accuracy: 0.7082 - loss: 0.6942 - val_accuracy: 0.8018 - val_loss: 0.5202\n",
      "Epoch 3/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 634ms/step - accuracy: 0.8007 - loss: 0.4939 - val_accuracy: 0.7973 - val_loss: 0.5069\n",
      "Epoch 4/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 626ms/step - accuracy: 0.8500 - loss: 0.4001 - val_accuracy: 0.8333 - val_loss: 0.4616\n",
      "Epoch 5/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 624ms/step - accuracy: 0.8480 - loss: 0.3404 - val_accuracy: 0.8288 - val_loss: 0.5048\n",
      "Epoch 6/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 624ms/step - accuracy: 0.8842 - loss: 0.2854 - val_accuracy: 0.8243 - val_loss: 0.4642\n",
      "Epoch 7/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 685ms/step - accuracy: 0.9189 - loss: 0.2295 - val_accuracy: 0.8468 - val_loss: 0.4658\n",
      "Epoch 8/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 628ms/step - accuracy: 0.9363 - loss: 0.1943 - val_accuracy: 0.8063 - val_loss: 0.5062\n",
      "Epoch 9/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 622ms/step - accuracy: 0.9109 - loss: 0.2157 - val_accuracy: 0.8108 - val_loss: 0.5514\n",
      "Epoch 10/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 624ms/step - accuracy: 0.9529 - loss: 0.1590 - val_accuracy: 0.8514 - val_loss: 0.4699\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "def create_mobilenet():\n",
    "    base_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False \n",
    "    \n",
    "    model1 = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model1.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model1\n",
    "\n",
    "\n",
    "model1 = create_mobilenet()\n",
    "\n",
    "\n",
    "history = model1.fit(x_train, y_train, \n",
    "                     epochs=10,\n",
    "                     batch_size=32, \n",
    "                     validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 1us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 2s/step - accuracy: 0.3413 - loss: 1.6623 - val_accuracy: 0.3919 - val_loss: 1.1242\n",
      "Epoch 2/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.3344 - loss: 1.2194 - val_accuracy: 0.3423 - val_loss: 1.0764\n",
      "Epoch 3/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.3872 - loss: 1.1356 - val_accuracy: 0.2973 - val_loss: 1.1384\n",
      "Epoch 4/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.3853 - loss: 1.1123 - val_accuracy: 0.4910 - val_loss: 1.0278\n",
      "Epoch 5/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.4097 - loss: 1.0660 - val_accuracy: 0.4324 - val_loss: 1.0579\n",
      "Epoch 6/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.3857 - loss: 1.0807 - val_accuracy: 0.4910 - val_loss: 1.0287\n",
      "Epoch 7/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2s/step - accuracy: 0.4034 - loss: 1.0610 - val_accuracy: 0.4955 - val_loss: 1.0099\n",
      "Epoch 8/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.4445 - loss: 1.0388 - val_accuracy: 0.5045 - val_loss: 1.0057\n",
      "Epoch 9/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 2s/step - accuracy: 0.4139 - loss: 1.0604 - val_accuracy: 0.4414 - val_loss: 1.0281\n",
      "Epoch 10/10\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 2s/step - accuracy: 0.4279 - loss: 1.0404 - val_accuracy: 0.4369 - val_loss: 1.0366\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - accuracy: 0.3344 - loss: 1.0452\n",
      "Test accuracy: 0.3484848439693451\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "def create_resnet50():\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model2 = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dense(1024, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model2.compile(optimizer='adam',\n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "    \n",
    "    return model2\n",
    "\n",
    "\n",
    "model2 = create_resnet50()\n",
    "\n",
    "\n",
    "history = model2.fit(x_train, y_train, \n",
    "                     epochs=10,\n",
    "                     batch_size=32, \n",
    "                     validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
